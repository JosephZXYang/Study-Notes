---
description: Week 2 of the course
---

# 2. Linear Regression with Multiple Variables

## Multivariate Linear Regression

* New notation
  * $$n=$$ number of features
  * $$x^{(i)}=$$ input feature vector of the $$i^{th}$$ training example
  * $$x_j^{(i)} = $$ the value of the $$j^{th}$$ feature in the $$i^{th}$$ training example
* New hypothesis
  * $$h_{\theta}(x) = \theta_0 + \sum_{i=1}^n \theta_i x_i$$
  * If we define $$x_0=1$$, we would have $$h_{\theta}(x) = \vec{\theta} \cdot \vec{x}$$
* Gradient descent for MLR
  * Update step: $$\theta_j := \theta_j - \alpha \frac{1}{m} \sum_{i=1}^m(h_{\theta}(x^{(i)}-y^{(i)})x_j^{(i)}$$
* **Feature Scaling**
  * Idea: Make sure the features are on a **similar scale**
  * If the features are not on the same scale, gradient descent could take long time to converge.
  * **Common trick**: get every feature into approximately a $$-1 \le x_i \le 1$$ range.
  * **Another approach - Mean normalization**
    * Replace $$x_i$$ with $$x_i-\mu_i$$ to make features have approximately zero mean
    * Then normalize the feature values to change the range to roughly $$(-1, 1)$$
* **Learning Rate**
  * \*\*\*\*

