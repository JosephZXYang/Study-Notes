# 6. Machine Learning System Design

## Advice for Applying Machine Learning

* **Improving a learning algorithm**
  * Get more training examples
  * Try smaller set of features
  * Try larger set of features
  * Add polynomial features
  * Change regularization parameter $$\lambda$$
* Each potential improvement might take months to find out whether they improve the performance
* **Machine learning diagnostic**
  * Might take long time to implement
  * But still could be great guidance of how to improve the hypothesis
* **Evaluating the hypothesis**
  * Split input set into
    1. Training set \(60%\)
    2. Cross Validation set \(20%\)
    3. Test set \(20%\)
  * If possible, select hypothesis according to cross validation error, and report the actual loss, which is the test error

## Bias vs. Variance

* **Bias vs. Variance**
  * **High bias:** high training error, high test/validation error \(underfit\)
  * **High variance:** low training error, high test/validation error \(overfit\)
* **Regularization and Bias/Variance**
  * $$\lambda$$too large, then parameters have very small numerical values, hence underfit
  * $$\lambda$$too small, then there is low cost of parameters' numerical values \(could be anything\), so overfit
* **Learning curve**
  * **Error** w.r.t. **training set size**
  * **Hypothesis too simple \(underfit\)**
    * If a learning algorithm is suffering from high bias, getting more training data will not help improving the performance
    * Training error starting low, getting higher gradually
    * Validation error staring high, getting lower gradually
    * These errors get close in the end, but still very high
    * **High bias**
  * **Hypothesis too complicated \(overfit\)**
    * If a learning algorithm is suffering from high variance, getting more training data is likely to help
* **Neural network**
  * **Small neural network** \(fewer layers, fewer neurons per layer\)
    * Prone to underfit
    * Computationally cheaper
  * **Large neural network** \(more layers, more neurons per layer\)
    * Prone to overfit
    * Computationally more expensive
    * Use regularization to address overfit
    * Normally, larger neural networks work better

## Building a Spam Classifier

* Word vector representation
* How to build a spam classifier
  * Collect lots of data
  * Develop sophisticated features based on email routing information \(from email header\)
  * Develop sophisticated features for message body
  * Develop sophisticated algorithm to detect intentional misspellings \(e.g. "m0rtgage", "med1cine", etc.\)
  * **It's hard to tell in advance which of the options will be most helpful.**
* **Recommended approach**
  * Start with a **simple** algorithm that you could implement quickly. Implement it and test on the cross-validation data
  * Plot learning curves to decide if more data, more features, etc. are likely to help
  * Error analysis: **manually examine** the examples \(in cross-validation set\) **that the algorithm made error on**. See if there's any **systematic trend** in what type of examples the algorithm is making errors on

## Handling Skewed Data

* **Cancer prediction example**
  * Suppose $$y=1$$is the **rare** class that we want to detect \(cancer\)
  * **Precision:** of all the patients where we predicted positive \($$y=1$$\), what fraction actually has cancer \(denominator: **predicted** positive\)
  * **Recall:** of all the patients that have cancer, what fraction did we correctly detect as having cancer \(denominator: **actual** positive\)
* **Precision/Recall tradeoff**
  * Predict positive if the result is greater or equal to some threshold
  * Logistic regression $$0\le h_{\theta}(x) \le 1$$
  * Threshold $$0\le T \le 1$$
  * $$y=1$$if $$h_{\theta}(x) \ge T$$
  * **Lower threshold - lower precision, higher recall**
  * **Higher threshold - higher precision, lower recall**
  * $$F_1$$score $$2\frac{PR}{P+R}$$

