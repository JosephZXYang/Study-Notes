# 6. Machine Learning System Design

## Advice for Applying Machine Learning

* **Improving a learning algorithm**
  * Get more training examples
  * Try smaller set of features
  * Try larger set of features
  * Add polynomial features
  * Change regularization parameter $$\lambda$$
* Each potential improvement might take months to find out whether they improve the performance
* **Machine learning diagnostic**
  * Might take long time to implement
  * But still could be great guidance of how to improve the hypothesis
* **Evaluating the hypothesis**
  * Split input set into
    1. Training set \(60%\)
    2. Cross Validation set \(20%\)
    3. Test set \(20%\)
  * If possible, select hypothesis according to cross validation error, and report the actual loss, which is the test error

## Bias vs. Variance

* **Bias vs. Variance**
  * **High bias:** high training error, high test/validation error \(underfit\)
  * **High variance:** low training error, high test/validation error \(overfit\)
* **Regularization and Bias/Variance**
  * $$\lambda$$too large, then parameters have very small numerical values, hence underfit
  * $$\lambda$$too small, then there is low cost of parameters' numerical values \(could be anything\), so overfit
* **Learning curve**
  * **Error** w.r.t. **training set size**
  * **Hypothesis too simple \(underfit\)**
    * If a learning algorithm is suffering from high bias, getting more training data will not help improving the performance
    * Training error starting low, getting higher gradually
    * Validation error staring high, getting lower gradually
    * These errors get close in the end, but still very high
    * **High bias**
  * **Hypothesis too complicated \(overfit\)**
    * If a learning algorithm is suffering from high variance, getting more training data is likely to help
* **Neural network**
  * **Small neural network** \(fewer layers, fewer neurons per layer\)
    * Prone to underfit
    * Computationally cheaper
  * **Large neural network** \(more layers, more neurons per layer\)
    * Prone to overfit
    * Computationally more expensive
    * Use regularization to address overfit
    * Normally, larger neural networks work better

