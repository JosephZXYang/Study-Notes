---
description: Week 2 of the course
---

# 2. Linear Regression with Multiple Variables

## Multivariate Linear Regression

* New notation
  * $$n=$$ number of features
  * $$x^{(i)}=$$ input feature vector of the $$i^{th}$$ training example
  * $$x_j^{(i)} = $$ the value of the $$j^{th}$$ feature in the $$i^{th}$$ training example
* New hypothesis
  * $$h_{\theta}(x) = \theta_0 + \sum_{i=1}^n \theta_i x_i$$
  * If we define $$x_0=1$$, we would have $$h_{\theta}(x) = \vec{\theta} \cdot \vec{x}$$
* Gradient descent for MLR
  * Update step: $$\theta_j := \theta_j - \alpha \frac{1}{m} \sum_{i=1}^m(h_{\theta}(x^{(i)}-y^{(i)})x_j^{(i)}$$
* **Feature Scaling**
  * Idea: Make sure the features are on a **similar scale**
  * If the features are not on the same scale, gradient descent could take long time to converge.
  * **Common trick**: get every feature into approximately a $$-1 \le x_i \le 1$$ range.
  * **Another approach - Mean normalization**
    * Replace $$x_i$$ with $$x_i-\mu_i$$ to make features have approximately zero mean
    * Then normalize the feature values to change the range to roughly $$(-1, 1)$$
* **Learning Rate**
  * Making sure gradient descent is working correctly
    * Plot $$J(\theta)$$against number of iterations.
    * $$J(\theta)$$should decrease after every iteration.
  * Example automatic convergence test:
    * declare convergence if $$J(\theta)$$decreases by less than $$10^{-3}$$ by each iteration \(but the best threshold might be hard to choose\).
  * It gradient descent is not working, **use smaller** $$\alpha$$.
  * Proven by mathematics: with **sufficiently small learning rate** $$\alpha$$, under certain assumptions of the cost function, gradient descent will always converge. 
  * But for very small learning rate, convergence could take a huge amount of time.
* **Features and Polynomial Regression**
  * Sometimes we might want a polynomial cost function \(quadratic, cubic, etc.\)
  * Instead of using one variable with different orders, we could use multiple variables with first order. Suppose $$x$$ is the original variable:
    * $$v_1 = x$$
    * $$v_2 = x^2$$
    * $$v_3 = x^3$$
  * But notice that, since the orders are different, the range of $$v_1, v_2, v_3$$ should be different. Hence when applying feature scaling, we would have \(suppose the range of $$x$$ is $$10^0-10^3$$:
    * $$u_1 = v_1/10^3$$
    * $$u_1 = v_2/10^6$$
    * $$u_3 = v_3/10^9$$

## Computing Parameters Analytically

* **Normal Equation**
  * \*\*\*\*



