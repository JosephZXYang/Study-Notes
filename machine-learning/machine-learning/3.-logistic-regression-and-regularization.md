---
description: Week 3 of the course
---

# 3. Logistic Regression and Regularization

## Classification and Representation

* Classification problems
  * Email: Spam / Not Spam
  * Online Transactions: Fraudulent / Not Fraudulent
  * Tumor: Malignant / Benign
* Binary Classification
  * $$y \in \big\{0, 1\big\}$$
* **Logistic Regression Model**
  * Want $$0\le h_{\theta}(x)\le1$$
  * $$h_{\theta}(x):=g(\theta^Tx)$$
  * $$g(z):=1/(1+e^{-z})$$, sigmoid \(a.k.a logistic\) function
* **Decision Boundary**
  * Suppose we predict
    * $$y=1$$if $$h_{\theta}(\theta^Tx) \ge 0.5$$
    * $$y=0$$if $$h_{\theta}(\theta^Tx) < 0.5$$
  * Notice that for sigmoid function $$g(z)$$, the result is greater than 0.5 if the input is positive and the result if smaller than 0.5 if the input is negative.
  * So we change the prediction to
    * $$y=1$$if $$\theta^Tx \ge 0$$
    * $$y=0$$if $$\theta^Tx < 0$$
  * The decision boundary is the line where $$\theta^Tx = 0$$
  * If a straight line **cannot** split the training examples, i.e. the order of $$\theta^Tx$$ is too low \(the polynomial is too simple to fit\), add higher order terms like $$x_1^2, x_2^2, x_1x_2^2, x_1^2x_2, x_1^3, x_2^3$$ until the polynomial of decision boundary is complex enough to split the training examples.

## Logistic Regression Model

* **Cost Function:**
  * $$\text{Cost}(h_{\theta}(x), y) := $$
    * $$-\log(h_{\theta}(x))$$ if $$y=1$$
    * $$-\log(1- h_{\theta}(x))$$ if $$y=0$$
  * Notice that:
    1. The cost is 0 if our prediction is perfectly correct \($$h_{\theta}(x)=y$$.
    2. But if our prediction is completely wrong \($$h_{\theta}(x)=1-y$$\), then we give a very hard penalization to our hypothesis \(the cost tends to infinity\).
* **Simplified Cost Function and Gradient Descent**
  * We combine the two cases and generate a simple cost function that works for both cases:
    * $$\text{Cost}(h_{\theta}(x), y) := -y \log(h_{\theta}(x)) - (1-y)\log(1-h_{\theta}(x))$$
  * $$J(\theta) = -\frac{1}{m} [\sum_{i=1}^m y^{(i)}\log h_{\theta}(x^{(i)})+(1-y^{(i)})\log (1-h_{\theta}(x^{(i)})]$$
  * Gradient Descent update step:
    * $$\theta := \theta - \frac{\alpha}{m} X^T(g(X\theta)-\vec{y}$$
* **Advanced Optimization**
  * Other optimization algorithms
    * Conjugate gradient 
    * BFGS
    * L-BFGS
  * Advantages:
    * No need to pick the learning rate $$\alpha$$
    * Often faster
  * Disadvantages:
    * More complex
* **Multiclass Classification**
  * Tagging problems with multiple possible tags
  * **One-vs-all \(one-vs-rest\) technique**:
    * Train a logistic classifier for each class.
    * When making prediction, pick the class that maximizes the result generated by corresponding hypothesis.

## Overfitting

* **Overfitting and Underfitting**
  * **Underfitting \(or High Bias\):** The hypothesis does not capture the pattern of training data well.
  * **Overfitting \(or High Variance\):** If we have too many features, the learned hypothesis may fit the training set very well, but fail to generalize ****to new examples.
  * Avoid overfitting
    1. Reduce number of features
    2. Regularization
* **Regularization and Cost Function**
  * Idea of regularization: we want small values for some parameters \(possibly all parameters\)
    * "Simpler" hypothesis
    * Less prone to overfitting

