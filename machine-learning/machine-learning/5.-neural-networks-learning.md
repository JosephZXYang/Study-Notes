# 5. Neural Networks: Learning

## Cost Function

* **Generalized version** of logistic regression
* For each training data, **sum** over the logistic regression cost **for each output class**
* Plus the regularization term. All entries of$$\Theta$$matrix for each layer

## Back-propagation Algorithm

* **Step 1: front-propagation**
  * Calculate the value for each neuron in the network
* **Step 2: back-propagation**
  * **Intuition:** $$\delta_j^{(l)}=$$"error" of node $$j$$ in layer $$l$$
  * **Output layer**
    * $$\delta_j^{(k)} = (h_{\Theta}(x))_j - y_j$$
  * **Hidden layers**
    * $$\delta^{(i)} = (\Theta^{(i)})^T\delta^{(i+1)}.*g'(z^{(i)})$$
    * $$g'(z^{(i)}) = a^{(i)} .* (1-a^{(i)})$$
* **Partial derivative terms** \(ignoring regularization\)
  * $$\frac{\partial}{\partial\Theta_ij^{(l)}}J(\Theta) = a_j^{(l)}\delta_i^{(l+1)}$$



