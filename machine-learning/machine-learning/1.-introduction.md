---
description: Week 1 content of the course
---

# 1. Introduction

## Introduction

> Machine learning is the science of getting computers to learn, without being explicitly programmed.

### Machine Learning

* Grew out of work in AI \(some work can't be done easily by AI: spam filtering, page ranking, etc.\)
* New capability for computers
  * GPU
* Examples:
  * Database mining: large datasets from growth of automation/web
  * Applications that can't be programmed by hand
  * Self-customizing programs \(Amazon, Netflix product recommendations\)
  * Understanding human learning \(human brain, real AI\)
* > A computer program is said to _learn_ from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E. -- Tom Mitchell \(1998\)
* Machine learning algorithms
  * Supervised learning
  * Unsupervised learning
  * Others:
    * Reinforcement learning
    * Recommender systems

### Supervised Learning

![](../../.gitbook/assets/image%20%282%29.png)

* Example: Housing price prediction \(**regression**\)
  * In this example, two possible predictions both feasible
    * Fit a straight line \(pink\)
    * Fit a quadratic line \(blue\)
  * This is called **supervised learning**, with the "_**right answers**_" given \(i.e. which general pattern we expect the output to look like.
  * This problem is also a **regression** problem: predict **continuous** valued output.



![](../../.gitbook/assets/image%20%283%29.png)

* Example: breast cancer classification prediction 
  * Setting: malignant = 1, benign = 0
  * This is a **classification** problem: **discrete** valued output.

### Unsupervised Learning

* Given **ONLY** a dataset, without any further information attached to it \(neither what the data points are, nor what to do with the data points\), try to find out some structure in the data
* Example problem: **Clustering problem**
  * Organize computing clusters
  * Social network analysis
  * Market segmentation
  * Astronomical data analysis
* Example problem: **Cocktail party problem**
  * Given several speakers speaking at the same time
  * Given several microphones placed at different positions in the room
  * Each microphone records one **certain combination** of **all** speakers
  * Try to separate voices of different speakers
  * This could be done in **one-line code** using machine learning algorithm!

## Linear Regression

* Notation
  * **m:** number of training examples
  * **x**:  input variable / features
  * **y**: output variable / target variable
  * $$(x, y)$$: one training example
  * $$(x^{(i)}, y^{(i)})$$: the $$i^{th}$$ training example

![](../../.gitbook/assets/image%20%284%29.png)

* Machine learning pipeline
  1. **Training set** prepared
  2. Choose a **learning algorithm**
  3. Generate a **hypothesis** from training set and chosen learning algorithm
  4. Make **predictions** using the hypothesis
* Linear regression formal representation
  * $$h_{\theta}(x) = \theta_0 + \theta_1x$$
  * Shorthand: $$h(x)$$
  * Linear regression with one variable
  * Also called **univariate** **linear regression**

### Cost Function

* Decides the performance of a hypothesis
* Idea: Choose $$\theta_0, \theta_1$$such that $$h_{\theta}(x)$$is close to $$y$$ for our training examples $$(x, y)$$.
* **Squared error cost function**: $$\min_{\theta_0, \theta_1} \frac{1}{2m}\sum_{i=1}^m(h_{\theta}(x^{(i)}) - y^{(i)})^2$$
* **The most commonly used one for regression problem**.

### Parameter Learning - Gradient Descent

*   Our problem
  * Problem setup
    * Have some function $$J(\theta_0, \theta_1)$$
    * Want $$min_{\theta_0, \theta_1} J(\theta_0, \theta_1)$$
  * Outline
    * Start with **some** $$\theta_0, \theta_1$$
    * Keep changing $$\theta_0, \theta_1$$ according to some criteria and finally approach the specific $$\theta_0, \theta_1$$ that minimizes the cost function
* **Gradient Descent**
  * Update step: $$\theta_j := \theta_j-\alpha\frac{\partial}{\partial \theta_j}J(\vec{\theta})$$
  * The update step must be **simultaneous**.
  * Keep updating until convergence
* Parameter setting
  * If $$\alpha$$ is too small, gradient descent can be very slow to converge
  * If $$\alpha$$ is too large, gradient descent could overshoot the minimum, even diverge.
* Gradient descent can converge to local minimum, even with the learning rate fixed. 
* As we approach a local minimum, the gradient is getting smaller. So automatically gradient descent will take smaller steps, without decreasing the learning rate.





