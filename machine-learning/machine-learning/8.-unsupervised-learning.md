# 8. Unsupervised Learning

## Clustering

* Group data points to different segments
* Data points do not have labels
* Unsupervised learning algorithm

## K-means Clustering

* Most widely used clustering algorithm
* **Steps**
  1. Randomize $$K$$ cluster centroids
  2. Go through all data points. Assign each point to its closest centroid. 
  3. Compute mean for each cluster. Move cluster centroids to their mean.
  4. Repeat step 2, 3 until convergence
* The loss function of K-means should be constantly decreasing w.r.t. more iterations. If the loss function is increasing, then the code must be buggy.
* Random initialization:
  * Randomly pick $$K$$ data points from all data points. Set the centroids to these points.
  * K-means could end up to different local optima due to different initialization
  * Solution: Run the algorithm with many different random initializations. Pick the one with lowest cost at the end.
* How to choose $$K$$
  * Elbow method
    * Plot the cost function against number of clusters
    * Find the turning point of rapid decrease and slow decrease
    * Not used very often
  * Based on a metric for future performance in actual usage

## Dimensionality Reduction

* **Motivation 1: Data Compression**
  * **Example**: $$x_1$$ measures length in inches while $$x_2$$ measures length in centimeters
  * Some features could be highly correlated
  * **Solution of example**: Project all data points onto a line to reduce 2D feature vectors to 1D feature vectors
* **Motivation 2: Data Visualization**
  * Visualization available in 2D or 3D plots

## Principle Component Analysis

* Reducing from $$R^n$$ to $$\mathbb{R}^k$$: find $$k$$ vectors such that the plane they spanned has minimum projection error
* **Linear Regression vs. PCA**
  * LR minimizes the **vertical** error
  * PCA minimizes the **projection** error
* **PCA algorithm**
  * Preprocessing step: feature scaling / mean normalization
  * Compute the covariance matrix
  * Find eigenvectors of the covariance matrix by singular value decomposition
  * Get the first $$k$$ eigenvectors if we want to reduce to $$\mathbb{R}^k$$

## Applying PCA

* **Choosing** $$k$$**, the number of principle components**
  * Typically, choose the smallest $$k$$ such that **"99% of variance is retained"**
  * $$\frac{\sum_{i=1}^m ||x^{(i)}-x^{(i)}_{approx}||^2}{\sum_{i=1}^m ||x^{(i)}||^2} \le 0.01$$
  * $$1-\frac{\sum_{i=1}^kS_{ii}}{\sum_{i=1}^nS_{ii}} \le 0.01$$
  * These two conditions are mathematically equivalent
* **Do not use PCA to prevent overfitting**
  * Because PCA throws away potential information
  * Use regularization instead

